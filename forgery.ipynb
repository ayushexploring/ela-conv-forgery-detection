{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ebb8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "819777ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(2)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a2cc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee99b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.callbacks import EarlyS\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ab17da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68ed6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ela_image(path, quality):\n",
    "    temp_filename = 'temp_file_name.jpg'\n",
    "    ela_filename = 'temp_ela.png'\n",
    "    \n",
    "    image = Image.open(path).convert('RGB')\n",
    "    image.save(temp_filename, 'JPEG', quality = quality)\n",
    "    temp_image = Image.open(temp_filename)\n",
    "    \n",
    "    ela_image = ImageChops.difference(image, temp_image)\n",
    "    \n",
    "    extrema = ela_image.getextrema()\n",
    "    max_diff = max([ex[1] for ex in extrema])\n",
    "    if max_diff == 0:\n",
    "        max_diff = 1\n",
    "    scale = 255.0 / max_diff\n",
    "    \n",
    "    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n",
    "    \n",
    "    return ela_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9342b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_image_path = 'C:/Users/ayush/Work/Image-Forgery/Dataset/CASIA2.0_Revised/Au/Au_ani_00001.jpg'\n",
    "# Image.open(real_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e353aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_ela_image(real_image_path, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d032d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_image_path = 'C:/Users/ayush/Work/Image-Forgery/Dataset/CASIA2.0_Revised/Tp/Tp_D_NRN_S_N_ani10171_ani00001_12458.jpg'\n",
    "# Image.open(fake_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64fff7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_ela_image(fake_image_path, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f1813",
   "metadata": {},
   "source": [
    "DENOISING REAL IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f280a5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_5584\\995466720.py:13: FutureWarning: `multichannel` is a deprecated argument name for `estimate_sigma`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  sigma_est=estimate_sigma(img_r,multichannel=True,average_sigmas=True)  #Noise estimation\n",
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_5584\\995466720.py:14: FutureWarning: `multichannel` is a deprecated argument name for `estimate_sigma`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  sigma_est=estimate_sigma(img_r,multichannel=True,average_sigmas=True)  #Noise estimation\n",
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_5584\\995466720.py:17: FutureWarning: `multichannel` is a deprecated argument name for `denoise_wavelet`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  img_bayes=denoise_wavelet(img_r,method='BayesShrink',mode='soft',wavelet_levels=3,\n",
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_5584\\995466720.py:22: FutureWarning: `multichannel` is a deprecated argument name for `denoise_wavelet`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  img_visushrink=denoise_wavelet(img_r,method='VisuShrink',mode='soft',sigma=sigma_est/3,wavelet_levels=5,\n",
      "C:\\Users\\ayush\\anaconda3\\envs\\py310\\lib\\site-packages\\pywt\\_multilevel.py:43: UserWarning: Level value of 5 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Color-image denoising\n",
    "from skimage.restoration import (denoise_wavelet,estimate_sigma)\n",
    "from skimage.util import random_noise\n",
    "# from sklearn.metrics import peak_signal_noise_ratio\n",
    "import skimage.io\n",
    "\n",
    "img_r=skimage.io.imread('C:/Users/ayush/Work/Image-Forgery/Dataset/CASIA2.0_Revised/Au/Au_ani_00001.jpg')\n",
    "img_r=skimage.img_as_float(img_r) #converting image as float\n",
    "\n",
    "#sigma=0.35 #noise\n",
    "#imgn=random_noise(img,var=sigma**2) # adding noise\n",
    "\n",
    "sigma_est=estimate_sigma(img_r,multichannel=True,average_sigmas=True)  #Noise estimation\n",
    "sigma_est=estimate_sigma(img_r,multichannel=True,average_sigmas=True)  #Noise estimation\n",
    "\n",
    "# Denoising using Bayes\n",
    "img_bayes=denoise_wavelet(img_r,method='BayesShrink',mode='soft',wavelet_levels=3,\n",
    "                          wavelet='coif5',multichannel=True,convert2ycbcr=True,rescale_sigma=True)\n",
    "\n",
    "\n",
    "#Denoising using Visushrink\n",
    "img_visushrink=denoise_wavelet(img_r,method='VisuShrink',mode='soft',sigma=sigma_est/3,wavelet_levels=5,\n",
    "                               wavelet='coif5',multichannel=True,convert2ycbcr=True,rescale_sigma=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89eacf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# psnr_noisy = cv2.PSNR(img_r,img_r)\n",
    "# psnr_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c03b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psnr_bayes = cv2.PSNR(img_r,img_bayes)\n",
    "# psnr_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5469555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psnr_visu = cv2.PSNR(img_r,img_visushrink)\n",
    "# psnr_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38e17718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting images\n",
    "# plt.figure(figsize=(30,30))\n",
    "\n",
    "# # plt.subplot(2,2,1)\n",
    "# # plt.imshow(img,cmap=plt.cm.gray)\n",
    "# # plt.title('Original Image',fontsize=30)\n",
    "\n",
    "# plt.subplot(2,2,1)\n",
    "# plt.imshow(img_r,cmap=plt.cm.gray)\n",
    "# plt.title('Noisy Image',fontsize=30)\n",
    "\n",
    "# plt.subplot(2,2,3)\n",
    "# plt.imshow(img_bayes,cmap=plt.cm.gray)\n",
    "# plt.title('Bayes Image',fontsize=30)\n",
    "\n",
    "# plt.subplot(2,2,4)\n",
    "# plt.imshow(img_visushrink,cmap=plt.cm.gray)\n",
    "# plt.title('Visushrink Image',fontsize=30)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7d1e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('PSNR[Original vs. Noisy Image]', psnr_noisy)\n",
    "# print('PSNR[Original vs. Denoised(VisuShrink)]', psnr_visu)\n",
    "# print('PSNR[Original vs. Denoised(Bayes)]', psnr_bayes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf4942",
   "metadata": {},
   "source": [
    "DENOISING FAKE IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1fe76d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_5584\\3535928763.py:13: FutureWarning: `multichannel` is a deprecated argument name for `estimate_sigma`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  sigma_est=estimate_sigma(img_f,multichannel=True,average_sigmas=True)  #Noise estimation\n",
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_5584\\3535928763.py:16: FutureWarning: `multichannel` is a deprecated argument name for `denoise_wavelet`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  img_bayes=denoise_wavelet(img_f,method='BayesShrink',mode='soft',wavelet_levels=3,\n",
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_5584\\3535928763.py:21: FutureWarning: `multichannel` is a deprecated argument name for `denoise_wavelet`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  img_visushrink=denoise_wavelet(img_f,method='VisuShrink',mode='soft',sigma=sigma_est/3,wavelet_levels=5,\n"
     ]
    }
   ],
   "source": [
    "# Color-image denoising\n",
    "from skimage.restoration import (denoise_wavelet,estimate_sigma)\n",
    "from skimage.util import random_noise\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "import skimage.io\n",
    "\n",
    "img_f=skimage.io.imread('C:/Users/ayush/Work/Image-Forgery/Dataset/CASIA2.0_Revised/Tp/Tp_D_NRN_S_N_ani10171_ani00001_12458.jpg')\n",
    "img_f=skimage.img_as_float(img_f) #converting image as float\n",
    "\n",
    "#sigma=0.35 #noise\n",
    "#imgn=random_noise(img,var=sigma**2) # adding noise\n",
    "\n",
    "sigma_est=estimate_sigma(img_f,multichannel=True,average_sigmas=True)  #Noise estimation\n",
    "\n",
    "# Denoising using Bayes\n",
    "img_bayes=denoise_wavelet(img_f,method='BayesShrink',mode='soft',wavelet_levels=3,\n",
    "                          wavelet='coif5',multichannel=True,convert2ycbcr=True,rescale_sigma=True)\n",
    "\n",
    "\n",
    "#Denoising using Visushrink\n",
    "img_visushrink=denoise_wavelet(img_f,method='VisuShrink',mode='soft',sigma=sigma_est/3,wavelet_levels=5,\n",
    "                               wavelet='coif5',multichannel=True,convert2ycbcr=True,rescale_sigma=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e8e0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# psnr_noisy = cv2.PSNR(img_f,img_f)\n",
    "# psnr_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6c43d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psnr_bayes = cv2.PSNR(img_f,img_bayes)\n",
    "# psnr_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6689902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psnr_visu = cv2.PSNR(img_f,img_visushrink)\n",
    "# psnr_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d20d2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting images\n",
    "# plt.figure(figsize=(30,30))\n",
    "\n",
    "# #plt.subplot(2,2,1)\n",
    "# #plt.imshow(img,cmap=plt.cm.gray)\n",
    "# #plt.title('Original Image',fontsize=30)\n",
    "\n",
    "# plt.subplot(2,2,1)\n",
    "# plt.imshow(img_f,cmap=plt.cm.gray)\n",
    "# plt.title('Noisy Image',fontsize=30)\n",
    "\n",
    "# plt.subplot(2,2,3)\n",
    "# plt.imshow(img_bayes,cmap=plt.cm.gray)\n",
    "# plt.title('Bayes Image',fontsize=30)\n",
    "\n",
    "# plt.subplot(2,2,4)\n",
    "# plt.imshow(img_visushrink,cmap=plt.cm.gray)\n",
    "# plt.title('Visushrink Image',fontsize=30)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6db380cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('PSNR[Original vs. Noisy Image]', psnr_noisy)\n",
    "# print('PSNR[Original vs. Denoised(VisuShrink)]', psnr_visu)\n",
    "# print('PSNR[Original vs. Denoised(Bayes)]', psnr_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "154481c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color-image denoising\n",
    "from skimage.restoration import (denoise_wavelet,estimate_sigma)\n",
    "from skimage.util import random_noise\n",
    "# from sklearn.metrics import peak_signal_noise_ratio\n",
    "import skimage.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "386abfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def denoise_img(img):\n",
    "#     #img=skimage.io.imread('C:/Users/prati/ImageForgeryDetection/Dataset/CASIA2/Tp/Tp_D_NRN_S_N_ani10171_ani00001_12458.jpg')\n",
    "#     img=skimage.img_as_float(img_f) #converting image as float\n",
    "\n",
    "\n",
    "#     sigma_est=estimate_sigma(img,multichannel=True,average_sigmas=True)  #Noise estimation\n",
    "\n",
    "#     # Denoising using Bayes\n",
    "#     img_bayes=denoise_wavelet(img,method='BayesShrink',mode='soft',wavelet_levels=3,\n",
    "#                           wavelet='coif5',multichannel=True,convert2ycbcr=True,rescale_sigma=True)\n",
    "\n",
    "\n",
    "#     #Denoising using Visushrink\n",
    "#     img_visushrink=denoise_wavelet(img,method='VisuShrink',mode='soft',sigma=sigma_est/3,wavelet_levels=5,\n",
    "#     wavelet='coif5',multichannel=True,convert2ycbcr=True,rescale_sigma=True)\n",
    "#     return img_bayes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d513d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (128, 128)\n",
    "def prepare_image(image_path):\n",
    "    return np.array(convert_to_ela_image(image_path, 90).resize(image_size)).flatten() / 255.0\n",
    "X = [] # ELA converted images\n",
    "Y = [] # 0 for fake, 1 for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8d4f59a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      8\u001b[0m     full_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, filename)\n\u001b[1;32m----> 9\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(\u001b[43mprepare_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m     Y\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(Y) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m, in \u001b[0;36mprepare_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_image\u001b[39m(image_path):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mconvert_to_ela_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\PIL\\Image.py:2192\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2184\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2185\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2186\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2187\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2188\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2189\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2190\u001b[0m         )\n\u001b[1;32m-> 2192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "count = 1\n",
    "path = r'C:/Users/ayush/Work/Image-Forgery/Dataset/CASIA2.0_Revised/Au/'\n",
    "for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        count += 1\n",
    "        if filename.endswith('jpg') or filename.endswith('png'):\n",
    "            full_path = os.path.join(dirname, filename)\n",
    "            X.append(prepare_image(full_path))\n",
    "            Y.append(1)\n",
    "            if len(Y) % 500 == 0:\n",
    "                print(f'Processing {len(Y)} images')\n",
    "            if count == 2064:\n",
    "                break\n",
    "\n",
    "random.shuffle(X)\n",
    "# X = X[:2100]\n",
    "# Y = Y[:2100]\n",
    "print(len(X), len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:/Users/ayush/Work/Image-Forgery/Dataset/CASIA2.0_Revised/Tp/'\n",
    "for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('jpg') or filename.endswith('png'):\n",
    "            full_path = os.path.join(dirname, filename)\n",
    "            X.append(prepare_image(full_path))\n",
    "            Y.append(0)\n",
    "            if len(Y) % 500 == 0:\n",
    "                print(f'Processing {len(Y)} images')\n",
    "\n",
    "print(len(X), len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47979da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array(X)\n",
    "Y = to_categorical(Y, 2)\n",
    "X = X.reshape(-1, 128, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c47ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, random_state=5)\n",
    "\n",
    "print(len(X_train), len(Y_train))\n",
    "print(len(X_val), len(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "528fbb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5, 5), padding = 'valid', activation = 'relu', input_shape = (128, 128, 3)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='valid', activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size =2, strides=None, padding='valid', data_format='channels_last'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e2cc13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 124, 124, 32)      2432      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 60, 60, 32)        25632     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 30, 30, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 30, 32)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 28800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               7373056   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,401,634\n",
      "Trainable params: 7,401,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda678c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "optimizer = tensorflow.keras.optimizers.legacy.RMSprop(learning_rate=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141b825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "x_train2 = np.array(X_train, copy=True)\n",
    "y_train2 = np.array(Y_train, copy=True)\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=10,\n",
    "    fill_mode='nearest',\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "print(type(X_train))\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='auto')\n",
    "\n",
    "validation_generator = datagen.flow(x_train2, y_train2, batch_size=batch_size, subset='validation')\n",
    "train_generator = datagen.flow(x_train2, y_train2,batch_size=batch_size, subset='training')\n",
    "\n",
    "# # fits the model on batches with real-time data augmentation:\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = (X_val,Y_val),\n",
    "                    verbose=2,\n",
    "                    callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-4\n",
    "optimizer = tensorflow.keras.optimizers.legacy.Adam(learning_rate = init_lr, decay = init_lr/epochs)\n",
    "model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor = 'val_loss',\n",
    "                              min_delta = 0,\n",
    "                              patience = 30,\n",
    "                              verbose = 0,\n",
    "                              mode = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba25da1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit(X_train,\n",
    "                 Y_train,\n",
    "                 batch_size = batch_size,\n",
    "                 epochs = epochs,\n",
    "                 validation_data = (X_val, Y_val),\n",
    "                 verbose=2,\n",
    "                 callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1936c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_casia_run1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a457d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(hist.history['loss'], color='b', label=\"Training loss\")\n",
    "ax.plot(hist.history['val_loss'], color='r', label=\"validation loss\")\n",
    "legend = ax.legend(loc='best', shadow=True)\n",
    "plt.title('Validation Loss = {loss}'.format(loss=hist.history['val_loss'][-1]))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(hist.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax.plot(hist.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax.legend(loc='best', shadow=True)\n",
    "plt.title('Validation Accuracy = {acc}'.format(acc=hist.history['val_accuracy'][-1]))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b983de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the loss and accuracy curves for training and validation \n",
    "# fig, ax = plt.subplots(2,1)\n",
    "# ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "# ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "# legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "# ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "# ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "# legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dab51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3272b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 124, 124, 32)      2432      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 60, 60, 32)        25632     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 30, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30, 30, 32)        0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 28800)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               7373056   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,401,634\n",
      "Trainable params: 7,401,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\"dot\" with args ['-Tps', 'C:\\\\Users\\\\ayush\\\\AppData\\\\Local\\\\Temp\\\\tmpeeb5xbxy'] returned code: 1\n",
      "\n",
      "stdout, stderr:\n",
      " b''\n",
      "b''\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_casia_run1.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_plot.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_layer_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\vis_utils.py:436\u001b[0m, in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m     )\n\u001b[1;32m--> 436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_graphviz\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    437\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install pydot (`pip install pydot`) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand install graphviz \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(see instructions at https://graphviz.gitlab.io/download/) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor plot_model to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython.core.magics.namespace\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;66;03m# We don't raise an exception here in order to avoid crashing\u001b[39;00m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;66;03m# notebook tests where graphviz is not available.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\vis_utils.py:57\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Attempt to create an image of a blank graph\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# to check the pydot/graphviz installation.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, pydot\u001b[38;5;241m.\u001b[39mInvocationException):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\pydot.py:1945\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1933\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1934\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{prog}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with args \u001b[39m\u001b[38;5;132;01m{arguments}\u001b[39;00m\u001b[38;5;124m returned code: \u001b[39m\u001b[38;5;132;01m{code}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1935\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout, stderr:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{out}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{err}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1941\u001b[0m         err\u001b[38;5;241m=\u001b[39mstderr_data,\n\u001b[0;32m   1942\u001b[0m     )\n\u001b[0;32m   1943\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message)\n\u001b[1;32m-> 1945\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m process\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, process\u001b[38;5;241m.\u001b[39mreturncode\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stdout_data\n",
      "\u001b[1;31mAssertionError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "model = load_model('model_casia_run1.h5')\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, ['fake','real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6113ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "score = metrics.accuracy_score(Y_true, Y_pred_classes)\n",
    "print(\"Accuracy: {}\".format(score))\n",
    "score = metrics.precision_score(Y_true, Y_pred_classes, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score))\n",
    "score = metrics.recall_score(Y_true, Y_pred_classes, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score))\n",
    "score_lr1 = metrics.f1_score(Y_true, Y_pred_classes, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_lr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming that you have true binary labels y_true and predicted scores y_score\n",
    "fpr, tpr, thresholds = roc_curve(Y_true, Y_pred_classes)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming that you have true binary labels y_true and predicted scores y_score\n",
    "fpr, tpr, thresholds = roc_curve(Y_true, Y_pred_classes)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['fake', 'real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0ed70",
   "metadata": {
    "scrolled": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# fake_image = os.listdir('C:/Users/ayush/Work/Image-Forgery/Dataset/CASIA2.0_Revised/Tp/')\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for file_name in fake_image:\n",
    "#     if file_name.endswith('jpg') or filename.endswith('png'):\n",
    "#         fake_image_path = os.path.join('C:/Users/ayush/Work/Image-Forgery/Dataset/CASIA2.0_Revised/Tp/', file_name)\n",
    "#         image = prepare_image(fake_image_path)\n",
    "#         image = image.reshape(-1, 128, 128, 3)\n",
    "#         y_pred = model.predict(image)\n",
    "#         y_pred_class = np.argmax(y_pred, axis = 1)[0]\n",
    "#         total += 1\n",
    "#         if y_pred_class == 0:\n",
    "#             correct += 1\n",
    "# #             print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total: {total}, Correct: {correct}, Acc: {correct / total * 100.0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e39b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# real_image = os.listdir('C:/Users/ayush/Work/Image-Forgery/Dataset/CASIA2.0_Revised/Au/')\n",
    "# correct_r = 0\n",
    "# total_r = 0\n",
    "# for file_name in real_image:\n",
    "#     if file_name.endswith('jpg') or filename.endswith('png'):\n",
    "#         real_image_path = os.path.join('C:/Users/ayush/Work/Image-Forgery/Dataset/CASIA2.0_Revised/Au/', file_name)\n",
    "#         image = prepare_image(real_image_path)\n",
    "#         image = image.reshape(-1, 128, 128, 3)\n",
    "#         y_pred = model.predict(image)\n",
    "#         y_pred_class = np.argmax(y_pred, axis = 1)[0]\n",
    "#         total_r += 1\n",
    "#         if y_pred_class == 1:\n",
    "#             correct_r += 1\n",
    "#             #print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct += correct_r\n",
    "total += total_r\n",
    "print(f'Total: {total_r}, Correct: {correct_r}, Acc: {correct_r / total_r * 100.0}')\n",
    "print(f'Total: {total}, Correct: {correct}, Acc: {correct / total * 100.0}')\n",
    "#print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ec70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe1b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
